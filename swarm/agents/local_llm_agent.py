"""
AI-–∞–≥–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ LLM –º–æ–¥–µ–ª—è–º–∏
"""

import asyncio
import json
from typing import Dict, Any, Optional

from .ai_agent_base import AIAgentBase, AIModelConfig
from ..core.agent import Task


class LocalLLMAgent(AIAgentBase):
    """
    AI-–∞–≥–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
    (Ollama, LM Studio, –ª–æ–∫–∞–ª—å–Ω—ã–µ Transformers –∏ —Ç.–¥.)
    """
    
    def __init__(
        self,
        model_name: str = "llama2",
        base_url: str = "http://localhost:11434",
        agent_id: Optional[str] = None,
        name: Optional[str] = None,
        system_prompt: Optional[str] = None,
        **kwargs
    ):
        config = AIModelConfig(
            model_name=model_name,
            base_url=base_url,
            timeout=60.0,  # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ
            **kwargs
        )
        
        super().__init__(
            model_config=config,
            agent_id=agent_id,
            name=name or f"Local-{model_name}",
            system_prompt=system_prompt,
            specialized_capabilities=["local_processing", "privacy_focused", "offline_capable"]
        )
        
    async def _call_ai_model(self, prompt: str, task: Task) -> Dict[str, Any]:
        """–í—ã–∑–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM –º–æ–¥–µ–ª–∏"""
        
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º
            
            # 1. Ollama API
            if "ollama" in self.model_config.base_url or ":11434" in self.model_config.base_url:
                return await self._call_ollama_api(prompt, task)
                
            # 2. LM Studio API
            elif "lmstudio" in self.model_config.base_url or ":1234" in self.model_config.base_url:
                return await self._call_lmstudio_api(prompt, task)
                
            # 3. OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API
            elif "/v1" in self.model_config.base_url:
                return await self._call_openai_compatible_api(prompt, task)
                
            # 4. –ü—Ä—è–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Transformers
            else:
                return await self._call_transformers_model(prompt, task)
                
        except Exception as e:
            self.ai_logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return await self._mock_local_response(prompt, task)
            
    async def _call_ollama_api(self, prompt: str, task: Task) -> Dict[str, Any]:
        """–í—ã–∑–æ–≤ Ollama API"""
        
        try:
            import aiohttp
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —É—á–µ—Ç–æ–º —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
            if self.conversation_history:
                recent_history = self.conversation_history[-6:]
                for msg in recent_history:
                    if msg["role"] in ["user", "assistant"]:
                        messages.insert(-1, {
                            "role": msg["role"],
                            "content": msg["content"]
                        })
            
            payload = {
                "model": self.model_config.model_name,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": self.model_config.temperature,
                    "num_predict": self.model_config.max_tokens
                }
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.model_config.base_url}/api/chat",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.model_config.timeout)
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        return {
                            "content": data.get("message", {}).get("content", ""),
                            "tokens_used": self._estimate_tokens(prompt, data.get("message", {}).get("content", "")),
                            "model": data.get("model", self.model_config.model_name),
                            "done": data.get("done", True),
                            "confidence": 0.8
                        }
                    else:
                        raise Exception(f"Ollama API –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status}")
                        
        except ImportError:
            self.ai_logger.warning("aiohttp –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É")
            return await self._mock_local_response(prompt, task)
        except Exception as e:
            self.ai_logger.error(f"–û—à–∏–±–∫–∞ Ollama API: {e}")
            return await self._mock_local_response(prompt, task)
            
    async def _call_lmstudio_api(self, prompt: str, task: Task) -> Dict[str, Any]:
        """–í—ã–∑–æ–≤ LM Studio API"""
        
        try:
            import aiohttp
            
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            payload = {
                "model": self.model_config.model_name,
                "messages": messages,
                "temperature": self.model_config.temperature,
                "max_tokens": self.model_config.max_tokens,
                "stream": False
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.model_config.base_url}/v1/chat/completions",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.model_config.timeout)
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        choice = data.get("choices", [{}])[0]
                        return {
                            "content": choice.get("message", {}).get("content", ""),
                            "tokens_used": data.get("usage", {}).get("total_tokens", 0),
                            "model": data.get("model", self.model_config.model_name),
                            "finish_reason": choice.get("finish_reason", "stop"),
                            "confidence": 0.85
                        }
                    else:
                        raise Exception(f"LM Studio API –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status}")
                        
        except Exception as e:
            self.ai_logger.error(f"–û—à–∏–±–∫–∞ LM Studio API: {e}")
            return await self._mock_local_response(prompt, task)
            
    async def _call_openai_compatible_api(self, prompt: str, task: Task) -> Dict[str, Any]:
        """–í—ã–∑–æ–≤ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ API"""
        
        try:
            import aiohttp
            
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            payload = {
                "model": self.model_config.model_name,
                "messages": messages,
                "temperature": self.model_config.temperature,
                "max_tokens": self.model_config.max_tokens
            }
            
            headers = {}
            if self.model_config.api_key:
                headers["Authorization"] = f"Bearer {self.model_config.api_key}"
                
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.model_config.base_url}/chat/completions",
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=self.model_config.timeout)
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        choice = data.get("choices", [{}])[0]
                        return {
                            "content": choice.get("message", {}).get("content", ""),
                            "tokens_used": data.get("usage", {}).get("total_tokens", 0),
                            "model": data.get("model", self.model_config.model_name),
                            "finish_reason": choice.get("finish_reason", "stop"),
                            "confidence": 0.8
                        }
                        
        except Exception as e:
            self.ai_logger.error(f"–û—à–∏–±–∫–∞ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ API: {e}")
            return await self._mock_local_response(prompt, task)
            
    async def _call_transformers_model(self, prompt: str, task: Task) -> Dict[str, Any]:
        """–ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Transformers"""
        
        try:
            # –≠—Ç–æ —Ç—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ transformers –∏ torch
            # –í production —Å—Ä–µ–¥–µ –ª—É—á—à–µ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å
            
            # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è Transformers
            await asyncio.sleep(3)  # –°–∏–º—É–ª—è—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            
            return {
                "content": f"–û—Ç–≤–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Transformers –Ω–∞: {prompt[:50]}...\n\n–≠—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é. –î–ª—è –ø–æ–ª–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ transformers –∏ pytorch.",
                "tokens_used": self._estimate_tokens(prompt, "response"),
                "model": self.model_config.model_name,
                "finish_reason": "stop",
                "confidence": 0.75
            }
            
        except Exception as e:
            self.ai_logger.error(f"–û—à–∏–±–∫–∞ Transformers –º–æ–¥–µ–ª–∏: {e}")
            return await self._mock_local_response(prompt, task)
            
    async def _mock_local_response(self, prompt: str, task: Task) -> Dict[str, Any]:
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        
        await asyncio.sleep(2)  # –°–∏–º—É–ª—è—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        if "code_analysis" in task.requirements:
            content = """–ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ (–ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å):

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥. –û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã:

‚úì –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–¥–∞ –ø–æ–Ω—è—Ç–Ω–∞
‚úì –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ö–æ—Ä–æ—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
‚úì –õ–æ–≥–∏–∫–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∞ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è:
- –î–æ–±–∞–≤–∏—Ç—å type hints –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
- –í–∫–ª—é—á–∏—Ç—å docstrings –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏:
- –ü–æ–ª–Ω–∞—è –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
- –ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
- –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ—Å—Ç—å –ø–æ–¥ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏"""

        elif "code_generation" in task.requirements:
            spec = task.content.get("specification", "—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å")
            content = f"""–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ (–ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å):

```python
# –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –¥–ª—è: {spec}

def local_generated_function(input_data):
    \"""
    –§—É–Ω–∫—Ü–∏—è, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é
    –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏: {spec}
    \"""
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if not input_data:
        return None
        
    # –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
    processed_data = process_locally(input_data)
    
    return {
        'result': processed_data,
        'status': 'success',
        'processed_by': 'local_model'
    }

def process_locally(data):
    \"""–õ–æ–∫–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\"""
    return f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ: {data}"
```

–ö–æ–¥ –≥–æ—Ç–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è!"""

        else:
            content = f"""–û—Ç–≤–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏:

–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≤–∞—à –∑–∞–ø—Ä–æ—Å –ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏.

–ó–∞–ø—Ä–æ—Å: {prompt[:100]}...

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏:
üîí –ü–æ–ª–Ω–∞—è –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å
‚ö° –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
üåê –†–∞–±–æ—Ç–∞ –æ—Ñ—Ñ–ª–∞–π–Ω
‚öôÔ∏è –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ—Å—Ç—å

–ì–æ—Ç–æ–≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–ª–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏."""

        return {
            "content": content,
            "tokens_used": self._estimate_tokens(prompt, content),
            "model": f"{self.model_config.model_name}-local",
            "finish_reason": "stop",
            "confidence": 0.8
        }
        
    def _estimate_tokens(self, prompt: str, response: str) -> int:
        """–ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞: ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        return len(prompt + response) // 4
        
    async def check_model_availability(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        
        try:
            # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏
            test_response = await self._call_ai_model("–ü—Ä–∏–≤–µ—Ç", Task(
                id="health_check",
                content={"description": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏"},
                requirements=["ai_processing"]
            ))
            
            return {
                "available": True,
                "model": self.model_config.model_name,
                "base_url": self.model_config.base_url,
                "response_time": test_response.get("response_time", 0),
                "status": "healthy"
            }
            
        except Exception as e:
            return {
                "available": False,
                "model": self.model_config.model_name,
                "base_url": self.model_config.base_url,
                "error": str(e),
                "status": "unhealthy"
            }
